# -*- coding: utf-8 -*-
"""NN_New_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dpfzTg0W3PYD-b-2YeTatgbbhySrrrdU
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score

from tensorflow.keras.datasets import mnist

tf.test.gpu_device_name()

(X_train,y_train),(X_test,y_test) = mnist.load_data()

y_cat_test = to_categorical(y_test,10)
y_cat_train = to_categorical(y_train,10)

X_train,X_test = X_train/255, X_test/255

print(y_train[19])
plt.imshow(X_train[19])

"""# DEFAULT MODEL
MODEL:  
layers - 1  
units - 128  
activation - relu  
loss - sparse_categorical_crossentropy  
optimizer - adam  
epochs - 10
"""

model_def = keras.Sequential([
                              keras.layers.Flatten(input_shape=(28,28)),
                              keras.layers.Dense(128,activation='relu'),
                              keras.layers.Dense(10,activation='softmax')
])

model_def.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
n_epochs=10
fit_def = model_def.fit(X_train,y_train,epochs=n_epochs,verbose=1)

eval_train_def = model_def.evaluate(X_train,y_train)

eval_test_def = model_def.evaluate(X_test,y_test)

"""# VARYING LAYERS
MODEL:  
layers - 1,2,4,8,10  
units - 128  
activation - relu  
loss - sparse_categorical_crossentropy  
optimizer - adam  
epochs - 10
"""

model_test_layer = []
model_train_layer = []
model_train_layer.append(eval_train_def[-1]*100)
model_test_layer.append(eval_test_def[-1]*100)
num_layer = [2,4,8,10]
for n in num_layer:
  model_x = keras.Sequential(keras.layers.Flatten(input_shape=(28,28)))
  for i in range(n):
    model_x.add(keras.layers.Dense(128,activation='relu',))
  model_x.add(keras.layers.Dense(10,activation='softmax'))
  model_x.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
  n_epochs=10
  fit_layer = model_x.fit(X_train,y_train,epochs=n_epochs,verbose=1)
  ev_test = model_x.evaluate(X_test,y_test)
  ev_train = model_x.evaluate(X_train,y_train)
  model_test_layer.append(ev_test[-1]*100)
  model_train_layer.append(ev_train[-1]*100)

model_test_layer, model_train_layer

plt.plot([1,2,4,8,10],model_train_layer,'b')
plt.plot([1,2,4,8,10],model_test_layer,'r')
plt.legend(['Training','Testing'])
plt.xlabel("Number of layers")
plt.ylabel("Accuracy (%)")
plt.ylim((96,100))
plt.title("Accuracy vs Number of Layers")

model_10layer = keras.Sequential([
                              keras.layers.Flatten(input_shape=(28,28))
])
for i in range(10):
  model_10layer.add(keras.layers.Dense(128,activation='relu'))
model_10layer.add(keras.layers.Dense(10,activation='softmax'))
model_10layer.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics='accuracy')
fitting = model_10layer.fit(X_train,y_train,epochs=50,verbose=1)
model_10layer.evaluate(X_train,y_train)
model_10layer.evaluate(X_test,y_test)
acc = fitting.history['accuracy']
acc = [100*x for x in acc]

plt.plot(acc,'b')
plt.title('Training Accuracy')
plt.xlabel('Epochs')
plt.ylim((90,100))
plt.ylabel('Accuracy')

"""# VARYING UNITS
MODEL:  
layers - 2  
units - 128  
activation - relu  
loss - sparse_categorical_crossentropy  
optimizer - adam  
epochs - 10
"""

model_test_units = []
model_train_units = []
num_units = [16,64,128,256,512]
for n in num_units:
  if n==128:
    model_test_units.append(model_test_layer[1])
    model_train_units.append(model_train_layer[1])
    continue
  model_x = keras.Sequential(keras.layers.Flatten(input_shape=(28,28)))
  model_x.add(keras.layers.Dense(n,activation='relu'))
  model_x.add(keras.layers.Dense(n,activation='relu'))
  model_x.add(keras.layers.Dense(10,activation='softmax'))
  model_x.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
  n_epochs=10
  model_x.fit(X_train,y_train,epochs=n_epochs,verbose=1)
  ev_test = model_x.evaluate(X_test,y_test)
  ev_train = model_x.evaluate(X_train,y_train)
  model_test_units.append(ev_test[-1]*100)
  model_train_units.append(ev_train[-1]*100)

model_test_units, model_train_units

plt.plot(num_units,model_train_units,'b')
plt.plot(num_units,model_test_units,'r')
plt.legend(['Training','Testing'])
plt.xlabel("Number of Units")
plt.ylabel("Accuracy (%)")
plt.ylim((95,100))
plt.title("Accuracy vs Number of Units")

"""# VARYING ACTIVATION FUNCTION
MODEL:  
layers - 2  
units - 256  
activation - relu, elu, tanh, sigmoid  
loss - sparse_categorical_crossentropy  
optimizer - adam  
epochs - 10
"""

model_test_activ = []
model_train_activ = []
activs = ['relu','elu','tanh','sigmoid']
for a in activs:
  if a == 'relu':
    model_test_activ.append(model_test_units[3])
    model_train_activ.append(model_train_units[3])
    continue
  model_x = keras.Sequential(keras.layers.Flatten(input_shape=(28,28)))
  model_x.add(keras.layers.Dense(256,activation=a))
  model_x.add(keras.layers.Dense(256,activation=a))
  model_x.add(keras.layers.Dense(10,activation='softmax'))
  model_x.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
  n_epochs=10
  model_x.fit(X_train,y_train,epochs=n_epochs,verbose=1)
  ev_test = model_x.evaluate(X_test,y_test)
  ev_train = model_x.evaluate(X_train,y_train)
  model_test_activ.append(ev_test[-1]*100)
  model_train_activ.append(ev_train[-1]*100)

model_test_activ, model_train_activ

plt.plot(activs,model_train_activ,'b')
plt.plot(activs,model_test_activ,'r')
plt.legend(['Training','Testing'])
plt.xlabel("Activation Function")
plt.ylabel("Accuracy (%)")
plt.ylim((96,100),)
plt.title("Accuracy vs Activation Function")

"""# VARYING BIAS
MODEL:  
layers - 2  
units - 256  
activation - relu  
loss - sparse_categorical_crossentropy  
optimizer - adam  
epochs - 10
"""

n_epochs = 10
model_test_bias = []
model_train_bias = []
model_test_bias.append(model_test_activ[0])
model_train_bias.append(model_train_activ[0])
#########################
model_01_bias = keras.Sequential([
                                  keras.layers.Flatten(input_shape=(28,28)),
                                  keras.layers.Dense(256,activation='relu'),
                                  keras.layers.Dense(256,activation='relu',use_bias=True),
                                  keras.layers.Dense(10,activation='softmax')
])
model_01_bias.compile(loss = 'sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model_01_bias.fit(X_train,y_train,epochs=n_epochs,verbose=1)
eval_test = model_01_bias.evaluate(X_test,y_test)
eval_train = model_01_bias.evaluate(X_train,y_train)
model_test_bias.append(eval_test[-1]*100)
model_train_bias.append(eval_train[-1]*100)
########################
model_10_bias = keras.Sequential([
                                  keras.layers.Flatten(input_shape=(28,28)),
                                  keras.layers.Dense(256,activation='relu',use_bias=True),
                                  keras.layers.Dense(256,activation='relu'),
                                  keras.layers.Dense(10,activation='softmax')
])
model_10_bias.compile(loss = 'sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model_10_bias.fit(X_train,y_train,epochs=n_epochs,verbose=1)
eval_test = model_10_bias.evaluate(X_test,y_test)
eval_train = model_10_bias.evaluate(X_train,y_train)
model_test_bias.append(eval_test[-1]*100)
model_train_bias.append(eval_train[-1]*100)
#########################
model_11_bias = keras.Sequential([
                                  keras.layers.Flatten(input_shape=(28,28)),
                                  keras.layers.Dense(256,activation='relu',use_bias=True),
                                  keras.layers.Dense(256,activation='relu',use_bias=True),
                                  keras.layers.Dense(10,activation='softmax')
])
model_11_bias.compile(loss = 'sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model_11_bias.fit(X_train,y_train,epochs=n_epochs,verbose=1)
eval_test = model_11_bias.evaluate(X_test,y_test)
eval_train = model_11_bias.evaluate(X_train,y_train)
model_test_bias.append(eval_test[-1]*100)
model_train_bias.append(eval_train[-1]*100)

model_test_bias,model_train_bias

plt.plot(['NB/NB','NB/B','B/NB','B/B'],model_train_bias,'b')
plt.plot(['NB/NB','NB/B','B/NB','B/B'],model_test_bias,'r')
plt.title("Accuracy vs Bias")
plt.xlabel("Bias/No-Bias")
plt.ylabel("Accuracy (%)")
plt.legend(['Training','Testing'])
plt.ylim((96,100))

"""# VARYING DROPOUT
MODEL:  
layers - 2  
units - 256  
activation - relu  
loss - sparse_categorical_crossentropy  
optimizer - adam  
epochs - 10  
dropout - none, 0.1, 0.3, 0.5
"""

model_test_dropout = []
model_train_dropout = []
model_test_dropout.append(model_test_bias[0])
model_train_dropout.append(model_train_bias[0])
dpout = [0.1,0.3,0.5]
for d in dpout:
  model_x = keras.Sequential(keras.layers.Flatten(input_shape=(28,28)))
  model_x.add(keras.layers.Dense(256,activation='relu'))
  model_x.add(keras.layers.Dropout(d))
  model_x.add(keras.layers.Dense(256,activation='relu'))
  model_x.add(keras.layers.Dropout(d))
  model_x.add(keras.layers.Dense(10,activation='softmax'))
  model_x.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
  n_epochs=10
  model_x.fit(X_train,y_train,epochs=n_epochs,verbose=1)
  ev_test = model_x.evaluate(X_test,y_test)
  ev_train = model_x.evaluate(X_train,y_train)
  model_test_dropout.append(ev_test[-1]*100)
  model_train_dropout.append(ev_train[-1]*100)

model_test_dropout, model_train_dropout

dpout = [0] + dpout
plt.plot(dpout,model_train_dropout,'b')
plt.plot(dpout,model_test_dropout,'r')
plt.title("Accuracy vs Dropout")
plt.xlabel("Dropout Ratio")
plt.ylabel("Accuracy (%)")
plt.ylim((96,100))

"""# VARYING LOSS FUNCTION
MODEL:  
layers - 2  
units - 256  
activation - relu  
loss - sparse_categorical_crossentropy, binary_crossentropy  
optimizer - adam  
epochs - 10  
dropout - 0.1
"""

model_test_loss = []
model_train_loss = []
# model_test_loss.append(model_test_dropout[1])
model_test_loss.append(98.25999736785889)
# model_train_loss.append(model_train_dropout[1])
model_train_loss.append(99.63666796684265)
model_x = keras.Sequential(keras.layers.Flatten(input_shape=(28,28)))
model_x.add(keras.layers.Dense(256,activation='relu'))
model_x.add(keras.layers.Dropout(0.1)),
model_x.add(keras.layers.Dense(256,activation='relu'))
model_x.add(keras.layers.Dropout(0.1)),
model_x.add(keras.layers.Dense(10,activation='softmax'))
model_x.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
n_epochs=10
model_x.fit(X_train,y_cat_train,epochs=n_epochs,verbose=1)
ev_test = model_x.evaluate(X_test,y_cat_test)
ev_train = model_x.evaluate(X_train,y_cat_train)
model_test_loss.append(ev_test[-1]*100)
model_train_loss.append(ev_train[-1]*100)

losses = ['sparse categorical\ncrossentropy','categorical\ncrossentropy']
# plt.figure()
plt.plot(losses, model_train_loss,'b')
plt.plot(losses, model_test_loss,'r')
plt.bar(losses, model_test_loss, alpha=0.2)
plt.ylim((96,100))
plt.legend(['Training','Test'])
plt.title("Accuracy vs Loss Function")
plt.xlabel("Loss Function")
plt.ylabel("Accuracy (%)")

"""# VARYING OPTIMIZER
MODEL:  
layers - 2  
units - 256  
activation - relu  
loss - sparse_categorical_crossentropy, binary_crossentropy  
optimizer - adam,rmsprop,adagrad,sgd  
epochs - 10  
dropout - 0.1
"""

model_test_optim = []
model_train_optim = []
model_test_optim.append(model_test_loss[0])
model_train_optim.append(model_train_loss[0])
opti = ['rmsprop','adagrad','sgd']
for o in opti:
  model_x = keras.Sequential(keras.layers.Flatten(input_shape=(28,28)))
  model_x.add(keras.layers.Dense(256,activation='relu'))
  model_x.add(keras.layers.Dropout(0.1)),
  model_x.add(keras.layers.Dense(256,activation='relu'))
  model_x.add(keras.layers.Dropout(0.1)),
  model_x.add(keras.layers.Dense(10,activation='softmax'))
  model_x.compile(loss='sparse_categorical_crossentropy',optimizer=o,metrics=['accuracy'])
  n_epochs=10
  model_x.fit(X_train,y_train,epochs=n_epochs,verbose=1)
  ev_test = model_x.evaluate(X_test,y_test)
  ev_train = model_x.evaluate(X_train,y_train)
  model_test_optim.append(ev_test[-1]*100)
  model_train_optim.append(ev_train[-1]*100)

model_test_optim, model_train_optim

opti = ['adam'] + opti
plt.plot(opti,model_train_optim,'b')
plt.plot(opti,model_test_optim,'r')
plt.legend(['Training','Testing'])
plt.title("Accuracy vs Optimizer")
plt.xlabel("Optimizer")
plt.ylabel("Accuracy (%)")

